<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Java on es Heavy Industries</title>
    <link>http://esheavyindustries.com/tags/java/</link>
    <description>Recent content in Java on es Heavy Industries</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Mar 2015 18:43:05 -0600</lastBuildDate>
    <atom:link href="http://esheavyindustries.com/tags/java/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Elasticsearch in Production, at Scale: One Man&#39;s Story</title>
      <link>http://esheavyindustries.com/2015/03/elasticsearch-in-production-at-scale-one-mans-story/</link>
      <pubDate>Tue, 24 Mar 2015 18:43:05 -0600</pubDate>
      
      <guid>http://esheavyindustries.com/2015/03/elasticsearch-in-production-at-scale-one-mans-story/</guid>
      <description>

&lt;h4 id=&#34;in-the-beginning:f90d37087c90d70a24c8fee54358486d&#34;&gt;In the Beginning&amp;hellip;&lt;/h4&gt;

&lt;p&gt;A few years ago I was part of a team that had a logging problem. We were part of an older style architecture and were charged with supporting all of the &amp;ldquo;middleware&amp;rdquo; which in this case, was pretty much everything except the databases or the physical servers.
This company was a fairly large company, one of the leaders in it&amp;rsquo;s industry so we dealt with a wide range of &amp;ldquo;middleware&amp;rdquo; services which each had different levels of criticality and volume. Everything from looking up a customer&amp;rsquo;s account details to submitting a request to have a return shipping box sent to the customer was in our world. Each one of these calls were logged in some form or fashion.
We actually had quite verbose logging as a lot of this information was gathered on a daily basis by our development teams who would then run a bunch of manual scripts and applications against the data. For that reason it was not unusual for a log message to be 2500kb or larger in some cases. Of course we also had the usual 100kb logs but I&amp;rsquo;d say our typical range was from 500-1500kb.&lt;/p&gt;

&lt;p&gt;Oh yeah, we had a ton of these coming through (as we&amp;rsquo;d find out later). In total we later came to find out we were doing about 1.5-2 terabytes of logging a day in production. All of this is to set the stage for the rest of this entry. I want you to understand the domain we were in as at the time, it was hard for us to find anyone doing near the amount of data we were with Elasticsearch. That&amp;rsquo;s not to say it wasn&amp;rsquo;t being done, the info just wasn&amp;rsquo;t being made public.&lt;br /&gt;
Sadly due to contractual obligations I am only now able to share how we solved our logging problem. I realize it has been a few years and things have changed but I think the core ideas and design behind what we achieved would still apply today. Plus I enjoy reading posts/stories from people that provide detail on how they solved a certain problem. It doesn&amp;rsquo;t always have to be the right one, but just how they did it.&lt;/p&gt;

&lt;h4 id=&#34;first-attempt:f90d37087c90d70a24c8fee54358486d&#34;&gt;First Attempt&lt;/h4&gt;

&lt;p&gt;So we had a logging problem, how were we going to solve it? Whatever we came up with had to be searchable, had to be near real-time and it had to scale.
We already had a product from a third party that we had paid a ton of money for that didn&amp;rsquo;t even work. The first time we tried it on just one of our servers it choked and gave up within 30 minutes so we knew that was not an option. Most people know about Splunk so we gave them a call and quickly realized that in order to support what we wanted we would end up backing an armored bank trunk up to their office every month. Needless to say they were not an option.&lt;/p&gt;

&lt;p&gt;I had some toy code from a previous project that had a concept of log parser -&amp;gt; queue -&amp;gt; log consumer -&amp;gt; database. This had never really gone anywhere but I felt that the basic design was fairly sound so I pitched it and was given a few weeks to pull something together to determine if this was possible.
One thing that was working in our favor is that we had just recently built a shiny new Apache Kafka cluster. This was for another project but was built to support IT as a whole so it was pretty beefy and just based on the nature of Kafka, scalable and redundant. Yes, we are dealing with &amp;ldquo;just logs&amp;rdquo; but these are production logs and we really don&amp;rsquo;t want to lose them.&lt;/p&gt;

&lt;p&gt;Ok cool, we know what the queuing system is going to be, what&amp;rsquo;s next? I honestly can&amp;rsquo;t remember why but for some reason the first decision was to use MongoDB + Elasticsearch, coupled together with a MongoDB river. Our logs would be inserted into MongoDB but we would use Elasticsearch for the indexing/searching functionality. This is where the story flow jumps around a bit because we did end up building out a POC with this back-end setup only to ditch it later.&lt;/p&gt;

&lt;p&gt;The main reason we ditched MongoDB + Elasticsearch was really just the un-necessary complexity that MongoDB added. We realized that it made sense to just treat Elasticsearch as our database and store everything directly in there, so that&amp;rsquo;s what we did.
Now we have a queuing application (Kafka) and a &amp;ldquo;database&amp;rdquo; (Elasticsearch), the next challenge was getting the log data into Elasticsearch. Our end goal was to have our applications write directly to Kafka and we even hacked up a simple Kafka log4j appender, but the powers that be were not comfortable with making such a large jump. It was decided that for our first trial phase we would keep writing logs to log files and then parsing those log files and sending the data to Elasticsearch.
This was before logstash became part of the Elasticsearch umbrella but it was still being described as a great pairing. The ELK stack as it came to be known was out there, it just came from three different people. We took a look at logstash but immediately hit a roadblock: it had no support for Kafka (at that time, I believe it does now). Logstash is written in ruby/jruby/whatever and for no real reason I had never really had an interest at writing ruby code. I can&amp;rsquo;t say anything bad about it because I honestly have never written one line of code in it, but at that moment I didn&amp;rsquo;t feel that learning ruby to write an add-on for something that might not even work for us was the best use of time, so we dropped logstash as an option.&lt;/p&gt;

&lt;p&gt;What did we end up going with? Our own in-house java applications that I really wish could have been open sourced but again, the people writing the checks were not really open to that idea. We ended up writing what we called our &amp;ldquo;log parser&amp;rdquo; that lived on the servers and sent the data from the log files to Kafka and then a &amp;ldquo;log consumer&amp;rdquo; that took the data off of Kafka and inserted it into Elasticsearch. Sounds simple enough right? Not so much, or at least not for us.&lt;/p&gt;

&lt;h4 id=&#34;second-attempt:f90d37087c90d70a24c8fee54358486d&#34;&gt;Second Attempt&lt;/h4&gt;

&lt;p&gt;We now have our technology stack at least picked out so it was time to start putting it together and see what we could do. There were a few design considerations that went into place around the parts we were creating. The main purpose of our log parser was to get data from the servers into Kafka as quickly as possible. We did some very loose formatting and clean-up of message but that was it. The idea was to spend as little time putting data into Kafka because that process was something we couldn&amp;rsquo;t really scale. We could always (at least that&amp;rsquo;s how we would design it) add more log consumers to keep up with incoming traffic but we couldn&amp;rsquo;t really add more log parsers to speed up the parsing -&amp;gt; Kafka stuff.&lt;/p&gt;

&lt;p&gt;The log parser itself was fairly basic, nothing super crazy outside of some simple regular expressions and threading. We ran multiple applications per physical server so we wrote it in a way that a single log parser could handle all logs on that server. This was setup via a config file which controlled what logs were monitored. From that point on it was basically just doing a &amp;ldquo;tail -F&amp;rdquo; on the logs and each time something was written it massaged it a bit and sent it on to Kafka.&lt;/p&gt;

&lt;p&gt;If the log parser was fairly simple the log consumer was ummm, a bit less simple. Since we had gone for speed on the log parser most of the heavy work was left to the log consumer. The good thing about our design was that we could horizontally scale this part in order to handle increased volume or just plain inefficiencies. For this round we knew there were a few things we wanted to break out of the message into their own Elasticsearch fields/terms. Things like the date, log level, class name, etc. In addition to that we had the log message itself which usually contained a bunch of other fields which bit us later on (more on that later). Being a big company with various development teams who all used their own logging formats we ended up with a mess of regex strings that we tested messages against to figure out where the fields we cared about were. On the surface this sounds really heavy and not very manageable, and at first it was. What we ended up building though was kind of a regex engine for only things we cared about that we could configure with a config file and that kept all of the compiled regular expressions in memory. It was still less than ideal but it got us what we needed and did it pretty quickly. Oddly enough I found out later that when they tried to replace the log parser with a version of logstash that supported Kafka, logstash was actually quite a bit slower. I don&amp;rsquo;t think this is a knock against logstash, it&amp;rsquo;s just that we had created essentially a customized, stripped down version of logstash built for only our logging environment.&lt;/p&gt;

&lt;p&gt;At this point we were in cruise control. All of the hard work had been done, we were able to pull messages off of Kafka as quickly as we could put them on and we had already started planning the celebration party. We just had to make the final hook into Elasticsearch and we&amp;rsquo;d be done. No problem, just toss together an Elasticsearch cluster, point the log consumers at it and sit back and watch. Wrong. Wrong.&lt;/p&gt;

&lt;p&gt;Elasticsearch is an awesome product, honestly. It&amp;rsquo;s one of the few apps I have used that takes a complex task and makes it super simple &lt;strong&gt;AND&lt;/strong&gt; it typically just works right out of the box. I was lucky enough to attend one of their first training classes and it&amp;rsquo;s obvious those guys/girls know their shit. Unfortunately for us we did not have any real world examples to study so when we built our Elasticsearch cluster from a random SWAG it totally missed the mark. Our consumption rates dropped a ton as soon as we pointed the log consumers at Elasticsearch and we were lucky to get a steady 1000 messages a second inserted. At first we thought it was the log consumers but as we researched it more we found the bottleneck was Elasticsearch. If I remember right I think we had four beefy indexing nodes (I want to say 120+gb of memory, a billion cores, etc) and then two &amp;ldquo;search&amp;rdquo; nodes that were a bit lighter.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s probably as good a time as ever to point out that a lot of the knowledge that Elasticsearch has provided users didn&amp;rsquo;t exist back then. That&amp;rsquo;s not anyone&amp;rsquo;s fault or a mark against Elasticsearch, it&amp;rsquo;s just what it is when it comes to open source products. Usually the documents are the last thing to get updated and given that Elasticsearch was still kind of flying low at this point, I don&amp;rsquo;t think a lot of people were pushing production traffic through it like we were so nobody was talking about the issues we were running into.&lt;/p&gt;

&lt;p&gt;Celebration party is cancelled, everyone is predicting failure and management is going on again about why we shouldn&amp;rsquo;t use open source products. In an attempt to right the ship we did exactly what you shouldn&amp;rsquo;t do with Elasticsearch: start turning random knobs and dials. We read through the documentation looking for any setting that sounded like it could potentially help and &amp;ldquo;tuned&amp;rdquo; it to some random number we pulled out of our ass. Sometimes it would improve things, but not drastically and usually it just made things worse. Another cool thing about Elasticsearch (we found out later) is that it has a lot of internal monitors to adjust/set things as it needs to based on the environment it&amp;rsquo;s currently in. We learned later that hardly ever do you need to change config settings outside of the basic stuff (networking, cluster name, etc). This is something that is now all over their site :)&lt;/p&gt;

&lt;p&gt;What do IT people do when they&amp;rsquo;ve run out of options and have no idea what they are going? Head to google.com! We spent a day or two searching for everything we could without any luck and eventually posted up a plea for help on the Elasticsearch mailing list. We got a few tips but nothing that we had already tried and again, the biggest issue was that nobody that was willing to respond had ever dealt with the volume we were doing. Luckily after a few days one of the nice Elasticsearch people reached out to me via E-mail to see how our testing was going. I think I had given my e-mail address on their website somewhere so it was pure luck that they reached out to me in the middle of our crisis. I explained the situation, provided the link to our mailing list posting and asked if they had any ideas. They actually got back to us and were willing to do a brief call and go over the issues we were seeing. Now I realize they are probably not going to do this for everyone who downloads their product, but at the time it was a glimmer of hope and we took full advantage of it. The engineer on the call gave us some great tips, all of which are on their site now, that did help out but it still didn&amp;rsquo;t get us where we needed to be. The bottom line was that we had a scaling problem. We simply did not have enough Elasticsearch resources to handle the data we were sending in so we needed to scale.&lt;/p&gt;

&lt;h4 id=&#34;third-attempt:f90d37087c90d70a24c8fee54358486d&#34;&gt;Third attempt&lt;/h4&gt;

&lt;p&gt;We&amp;rsquo;re all feeling better now that we&amp;rsquo;re not total failures and that we have a solvable solution. The question was how much was it going to take to solve it. One of the tips that came from our call with Elasticsearch was to shrink our heap down a bit and avoid using the G1 collector. They also mentioned that you typically want to dedicate half of your memory to file system cache (again, this is all on their site now). We ended up setting our heap at 30gb which in turn allowed us to run two Elasticsearch nodes on one box while still meeting their suggestions. In one config change we had just doubled our cluster size.
The next change we made was moving to local disks instead of using our SAN. I can&amp;rsquo;t tell you what SAN we were using or what the infrastructure looked like between the servers and the SAN, but that single change gave us the most improvement overall. Before we went to local disks we tried all types of kernel and scheduler tuning and just couldn&amp;rsquo;t get the numbers we needed. We ended up just putting in a shit ton of disks and partitioned them up into two groups, one group for each Elasticsearch node running on the server. No special RAID or anything, just a bunch of plain, formatted mounted disks and we let Elasticsearch do the load balancing if you will across them.&lt;/p&gt;

&lt;p&gt;Now we were getting somewhere! We were able to sustain 20k messages a second and handled spikes up to around 40k a second. We were basically consuming log messages near real-time (15ish second delay usually) and things seemed to be humming along. The celebration party was re-scheduled and everyone started giving high fives.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Time in GC is going up really fast&amp;rdquo; is something you don&amp;rsquo;t like to hear when supporting Java apps but that&amp;rsquo;s what I had just heard my co-worker tell me. He was talking about one of our Elasticsearch nodes and my heart sank. It turns out that while we had solved the indexing/input problem, we had never up to this point really used the search portion much. Previously the app was in such flux that most of the team had not been using it but now that it was stable, everyone was using it. Even people from other teams. Teams who wanted business data for an entire month based on a single term that tended to be in those 2000+kb log messages. Those familiar with Elasticsearch can probably already tell where this is going but basically we did not have enough resources in our cluster to support intense queries like this. The behind the scenes stuff that happens with Elasticsearch/Lucene based on these huge wildcard type searches were choking the cluster. We named this style of query &amp;ldquo;the query of death&amp;rdquo; because it always brought something down. You have to remember that we are talking over a terabyte of data per day x ~30 = lots of shit that needs to be put into memory.&lt;/p&gt;

&lt;p&gt;By this point our &amp;ldquo;couple of weeks&amp;rdquo; project had been going on for 3+ months and although the initial hardware we were given was free in the sense that it came from a retired project, we were a bit hesitant to go back to the well asking for more machines. The short-term solution was two pronged. First we had to educate our users on how to more efficiently use Elasticsearch + Kibana. For the IT side this wasn&amp;rsquo;t too big of a challenge but on the business side (how did they even end up in this discussion? These are system logs after all) it was a little bit more difficult.
They wanted a Google like search that just worked and with our current setup that wasn&amp;rsquo;t possible yet.
The second part of our short-term solution was to take our storage from 30 days down to 7. We were keeping a daily index in Elasticsearch so we just adjusted our clean-up script to basically set a hard limit on how much data you could shove into a query. This seemed to keep everyone happy for a bit and got us some extra time to research a bit more on other possible solutions. The end goal had always been to have 30 days of data so 7 days was not a permanent solution.&lt;/p&gt;

&lt;p&gt;I think it was around this time that I stumbled across a video from the Loggly guys where they had done a talk about their infrastructure. After watching their video I felt a bit of relief. We were starting to wonder if what we were doing was even possible with our setup, only to see this video where a real company who makes real money selling their logging product had an infrastructure almost identical to ours. They of course have their own custom log parser and log consumer, but they were doing the whole Kafka and Elasticsearch dance, plus they were doing it on AWS. If they could do it on AWS we should surely be able to do it with physical hardware.&lt;/p&gt;

&lt;p&gt;Back to solving our search problem&amp;hellip; The first thing we tried to do was go after some of the more common terms that people were searching/aggregating on and pull those out into their own terms. This way Elasticsearch didn&amp;rsquo;t have to load up the entire message when people from the business were doing their monthly reports or creating their business metric dashboards. We got a list of the most common terms they were searching on and went to work on the log consumer adding in even more regex-fu to help build built out more fine-grained documents for Elasticsearch. This helped us out when it came to the queries we knew were coming our way, but we couldn&amp;rsquo;t account for every strange query that was going to be sent so the only real solution was to expand our cluster. Again.&lt;/p&gt;

&lt;h4 id=&#34;fourth-attempt:f90d37087c90d70a24c8fee54358486d&#34;&gt;Fourth attempt&lt;/h4&gt;

&lt;p&gt;Unfortunately it was at this point where my time with this company came to an end. It was a bittersweet moment as this project had been one of the most challenging but rewarding projects I had been apart of. I did keep in touch with a few of the people I worked with on the project and found out that they were able to essentially double the size of the cluster and go back to 30 days worth of storage. The query of death would still pop up but it was not a weekly event anymore and it was something that was easily handled.
I realize that is a fairly abrupt end to the story without a real success or failure indication but hopefully it will help someone out there who is researching building their own logging solution for terabytes of data a day.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>