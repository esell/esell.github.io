<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zookeeper on es Heavy Industries</title>
    <link>http://esell.github.io/tags/zookeeper/</link>
    <description>Recent content in Zookeeper on es Heavy Industries</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 31 Jan 2015 22:02:17 -0700</lastBuildDate>
    <atom:link href="http://esell.github.io/tags/zookeeper/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Wrangling Zookeeper</title>
      <link>http://esell.github.io/2015/01/wrangling-zookeeper/</link>
      <pubDate>Sat, 31 Jan 2015 22:02:17 -0700</pubDate>
      
      <guid>http://esell.github.io/2015/01/wrangling-zookeeper/</guid>
      <description>

&lt;h5 id=&#34;spof:12b272660e693d03fef595b262170b60&#34;&gt;SPOF&lt;/h5&gt;

&lt;p&gt;SPOF = Single Point of Failure and we had one. It wasn&amp;rsquo;t a traditional failure point like having a critical service running on a single server or something, it was the fact that we had no way to auto-scale, or at least automate spinning up, one of our key components: &lt;a href=&#34;http://zookeeper.apache.org/&#34;&gt;Apache Zookeeper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;figure&gt;
  &lt;img src=&#34;http://esell.github.io/pics/zookeeper_small.gif&#34; alt=&#34;zookeeper&#34;&gt;
  &lt;figcaption&gt;&lt;b&gt;smug bastard&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;As we had been drawing up the architecture for our new environment one of the requirements is everything must auto-scale or be push button. Since we will be working in AWS, the architecture has been designed to handle failures across the board. Having something that can&amp;rsquo;t self heal or be re-provisioned without human intervention is dumb and would break our entire goal. There have been too many times where I&amp;rsquo;ve heard &amp;ldquo;So everything is automated&amp;hellip; except this one part that fails all the time and we have to pay a team to monitor 24 hours a day. Oh and by the way that part that fails all the time is key to everything else working&amp;rdquo;. Unfortunately Zookeeper is a bit old in it&amp;rsquo;s ways and still requires a few areas of manual configuration and taking it out of our stack was not an option so what options did we have?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Accept that we couldn&amp;rsquo;t automate this part and that despite our grand idea of what could be we would just build the same hodgepodge of shit everyone else had.&lt;/li&gt;
&lt;li&gt;Have a go with &lt;a href=&#34;https://github.com/Netflix/exhibitor&#34;&gt;Exhibitor&lt;/a&gt;. Netflix built it so it&amp;rsquo;s gotta be good right?&lt;/li&gt;
&lt;li&gt;Build our own solution&lt;/li&gt;
&lt;li&gt;Watch cat videos on youtube&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A combination of options 3 and 4 is what we ended going with (actually option 4, option 4 some more and then option 3. That led to some more of option 4). Why not option 2? While I&amp;rsquo;m sure Exhibitor is a fantastic app, it just didn&amp;rsquo;t feel right to us. The ideal way to use Exhibitor is with S3 which means we are relying on an AWS service. Our goal is to handle failures across the board so if we have a Zookeeper EC2 instance fail and a regional S3 failure we&amp;rsquo;re fucked. We could probably build around that but it just adds another stack of shit to keep track of. Additionally with Exhibitor all of the Zookeeper management is now done through Exhibitor. This could be a good thing I guess but we didn&amp;rsquo;t want yet another management tool being thrown into the mix.&lt;/p&gt;

&lt;p&gt;Ok, enough gum flapping, here is how we solved the question of the century: how do you automate scaling Zookeeper?&lt;/p&gt;

&lt;p&gt;Enter &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt; and &lt;a href=&#34;https://github.com/kelseyhightower/confd&#34;&gt;confd&lt;/a&gt; (plus some custom &lt;a href=&#34;http://golang.org&#34;&gt;Go&lt;/a&gt;)!&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;figure&gt;
  &lt;img src=&#34;http://esell.github.io/pics/jackpot.jpg&#34; alt=&#34;jackpot&#34;&gt;
  &lt;figcaption&gt;&lt;b&gt;blammo!&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We had already been using etcd for service discovery and minor random things we probably shouldn&amp;rsquo;t be using it for so it was already part of our stack. Additionally our etcd groupings had already been built so they fit our requirement of being able to handle catastrophic failure. Confd had been on the radar for another portion of our architecture so we were familiar with it and thought we&amp;rsquo;d toss it in the mix and see what happens. What happened was pure magic! The ability to add and remove Zookeeper instances without any human configuration and no additional AWS services to lean on. I guess it wasn&amp;rsquo;t pure magic but it was pretty cool.&lt;/p&gt;

&lt;p&gt;So how does this whole thing work? I&amp;rsquo;m glad you asked:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First we wrote a startup wrapper in Go for Zookeeper. At a high level what this wrapper does is control the &amp;ldquo;myid&amp;rdquo; file for the Zookeeper instance. There are various checks in there to make sure that one instance only gets one, previously unused ID. This information is published to etcd. On first start there is no cluster config so this new instance has an id but isn&amp;rsquo;t actually joined to anything.&lt;/li&gt;
&lt;li&gt;After we had figured out how to publish the information for everyone, we wrote up a quick confd template that builds out the &amp;ldquo;zoo.cfg&amp;rdquo; cluster details. This is the lame part because the number associated with the entry must match the &amp;ldquo;myid&amp;rdquo; of that host. So when you see &amp;ldquo;server.2=zoo2:2888:3888&amp;rdquo; in the config file, zoo2 better have an id of 2 or else shit might go down (literally). Once the config has been populated confd restarts Zookeeper. Confd is always checking in with etcd and updating the &amp;ldquo;zoo.cfg&amp;rdquo; file as needed.&lt;/li&gt;
&lt;li&gt;We have another tiny Go app that when it has free time, checks the entries in etcd to make sure they are accurate and removes any dead hosts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So yeah, that&amp;rsquo;s about it, nothing earth shattering by any means but a possible solution for some of you who are looking to do the same thing and for whatever reason are not interested in Exhibitor.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>