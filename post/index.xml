<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on es Heavy Industries</title>
    <link>http://esheavyindustries.com/post/</link>
    <description>Recent content in Posts on es Heavy Industries</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Jan 2016 19:16:35 -0700</lastBuildDate>
    <atom:link href="http://esheavyindustries.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Deploying the Latest Hot App on AWS for Free*</title>
      <link>http://esheavyindustries.com/2016/01/deploying-the-latest-hot-app-on-aws-for-free/</link>
      <pubDate>Fri, 01 Jan 2016 19:16:35 -0700</pubDate>
      
      <guid>http://esheavyindustries.com/2016/01/deploying-the-latest-hot-app-on-aws-for-free/</guid>
      <description>

&lt;h3 id=&#34;wait-what:028da4a231a5317af863941c3c91d223&#34;&gt;Wait, what?&lt;/h3&gt;

&lt;p&gt;AWS (Amazon Web Services) provides a free tier for new customers that allows you to play around with most of their services for a year without paying a cent. If you did not know that, you should head over to their page &lt;a href=&#34;https://aws.amazon.com/free/&#34;&gt;here&lt;/a&gt; and take a look at the details.&lt;/p&gt;

&lt;p&gt;This free tier is great for people who want to play around with the various AWS offerings without having to commit to anything. It is also a great way to deploy a new web app which is what I recently did and will cover in this post.&lt;/p&gt;

&lt;p&gt;Understand that what I&amp;rsquo;m talking about here is a bare-bones infrastructure that is good for low-volume, single region setups. This setup is NOT what you would typically use in your real production infrastructure but because of the way AWS is setup, you could easily scale out to a more fault tolerant architecture later without much work.&lt;/p&gt;

&lt;h3 id=&#34;free:028da4a231a5317af863941c3c91d223&#34;&gt;&amp;ldquo;Free&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;Everything I talk about in this article was done using the free AWS tier except for the domain name. So when I say free I am not including the cost of purchasing a domain name but that is also technically not something you need in order to make this work.&lt;/p&gt;

&lt;p&gt;Also I think it&amp;rsquo;s worth mentioning again that this architecture works for your initial site or your proof-of-concept. Don&amp;rsquo;t expect to run an app that is taking millions of hits a day on the free tier.&lt;/p&gt;

&lt;h3 id=&#34;background:028da4a231a5317af863941c3c91d223&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;I had a pet project of mine where the goal was to create an online wish list site that was basic, fast and just did what it was told. I wanted to create something that was super simple to use so that anyone in your family could use it. As this was a pet project I also didn&amp;rsquo;t want to put in a lot of time supporting it or babysitting it. I wanted it to &amp;ldquo;just work&amp;rdquo; without me having to setup a metric ass ton of systems to support it.&lt;/p&gt;

&lt;p&gt;The end result came out as &lt;a href=&#34;https://thewishler.com&#34;&gt;The Wishler&lt;/a&gt;. Feel free to check it out, it&amp;rsquo;s a live app and works as advertised so go nuts.&lt;/p&gt;

&lt;p&gt;Many of the things I ended up doing are just basic AWS best practices.&lt;/p&gt;

&lt;h3 id=&#34;how-i-did-it:028da4a231a5317af863941c3c91d223&#34;&gt;How I did it&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This is not meant to be a step-by-step tutorial but more of just a high-level overview on how one person stitched together various AWS offerings. The AWS documentation is pretty good so if you are new to AWS I&amp;rsquo;d suggest checking those out.&lt;/p&gt;

&lt;p&gt;I knew at the core what I wanted for The Wishler (an app server and a database) but the overall process was very iterative. I think this is one area where AWS really shines compared to other cloud providers. They have such a large catalog of offerings that you can basically hit a spot during your development where you think &amp;ldquo;I could really use something that did _____&amp;rdquo; and then jump onto their site and find a solution that matches up well.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s start at the beginning then: the app server. Since I was determined to stick to the free tier I decided to go with a standalone EC2 instance behind an Elastic Load Balancer. My actual app is a great candidate for Docker but with the way the EC2 Container service works you need to run your containers on EC2 instances. The only instance type that is free however is the t2.micro which I felt was not a great candidate for multiple containers.&lt;/p&gt;

&lt;p&gt;Now that I knew what I was going to run my app on, I needed to come up with a way that would allow it to scale when the time comes. Since I didn&amp;rsquo;t want to support additional load balancers and I really didn&amp;rsquo;t want to come up with the glue to bring the process together I went with an off-the-shelf AWS Auto Scaling group and Elastic Load Balancer. The beauty in these (for me) is that if I outgrow the free tier for some reason I&amp;rsquo;ll easily be able to scale pretty quickly. An example would be that my tiny t2.micro app server can no longer handle all the requests coming in. With an API call or a click of a button I can spin up a couple more t2.micro instances behind the already existing Elastic Load Balancer. No configuration changes, no IPs to keep track of just a simple way to handle growth.&lt;/p&gt;

&lt;p&gt;In order to use Auto scaling groups you&amp;rsquo;ll either need to create a base image that has something on it to install your app during start-up or use the user-data option. My app is written in Go and is pretty simple so I decided to lean some more on the free tier from AWS and use their CodeCommit product which is basically just a remote git repo. On start-up the instance will install the required Go tooling, checkout the master branch from CodeCommit, build it and start it up. There were quite a few options here that would have worked as well (creating a base AMI, keeping a deb file in S3, etc) but this gave me an excuse to play with another AWS product.&lt;/p&gt;

&lt;p&gt;At this point I now had an ELB pointed at my ASG which meant users could reach the site&amp;hellip; if they knew how to get to it. Using IPs to reach sites is not very popular these days so I decided to register a new domain &lt;a href=&#34;https://thewishler.com&#34;&gt;thewishler.com&lt;/a&gt; and instead of using my registrars DNS tools use the AWS Route53 product. Route53 makes it super simple to plugin to existing AWS infrastructure and also supports some more advanced DNS level load balancing such as geo-location and latency. I should also point out that I did lie a bit and Route53 will cost you about $0.90 a month assuming you have one zone (domain) and keep under 1 million DNS queries. Once Route53 was setup I simply pointed my domain at my already existing ELB and ta-da!&lt;/p&gt;

&lt;p&gt;Alright, so now I was at a point where I could serve up static pages so it was time to look at what I was going to use for the backend/database. I decided to go with Redis doing a rolling hourly snapshot to S3 which might seem kind of odd so let me explain. I am forward thinking so I wanted something that was going to easily scale not only across availability zones, but also across regions. When I say &amp;ldquo;easily&amp;rdquo; I mean something that from a maintenance and configuration standpoint is easy. I think for my specific application an RDBMS like MySQL or Postgres would have been a good fit but my experience with getting them to easily scale across regions has been more work than I wanted to put in.
Additionally the data I am dealing with is not what I consider critical data. If there is a complete failure and the database needs to be restored, missing an hour worth of data is not going to be the end of the world (in my opinion). On top of that Redis is fast which was a big factor for me.
Back on track&amp;hellip; So Redis is my backend which meant that I could use the AWS ElastiCache offering. ElastiCache offers either Memcached or Redis for the engine type so it was a great fit for me. I can still develop against a local Redis instance and then deploy to AWS without having to change anything. Additionally ElastiCache is kind of like an ASG for Redis. If I need to I can increase the number of instances in the cluster, scale them out across availability zones, create replication groups, etc. All of this is done via a few button clicks or the API and I don&amp;rsquo;t have to manage individual instances or installs which is really &lt;strong&gt;really&lt;/strong&gt; nice.
With the goal of keeping things free I just went with a single cache.t2.micro instance and called it a day.&lt;/p&gt;

&lt;p&gt;I was feeling pretty good now because I had the core of my app up and running in a somewhat robust setup in the sense that if an instance died off or failed it would be replaced with minimum downtime and it was all free*. I also didn&amp;rsquo;t have to manage a damn thing and actually didn&amp;rsquo;t even know the IP of any instance in my existing setup, all I cared about was pushing code to CodeCommit aka git and refreshing the ASG to pick-up the changes.&lt;/p&gt;

&lt;p&gt;The code was coming along well until I hit a point where I realized I would have to occasionally send e-mails to people. As part of &lt;a href=&#34;https://thewishler.com&#34;&gt;The Wishler&lt;/a&gt; password reset process the app sends out a unique reset ID via e-mail. Given that these e-mails are related to password resets I wanted them to hit the inbox and not get flagged as spam. I also did not want to manage an SMTP server. Guess what? AWS has an app for that called SES that allows you to send e-mails either via an API or plain ol SMTP using the AWS mail servers. This means another thing I don&amp;rsquo;t have to manage AND the servers are on IPs that are not going to get flagged as spam. The setup was super simple and since I was already using Route53 all of the DKIM stuff was setup for me.&lt;/p&gt;

&lt;h3 id=&#34;and-then:028da4a231a5317af863941c3c91d223&#34;&gt;And then&amp;hellip;&lt;/h3&gt;

&lt;p&gt;Now I&amp;rsquo;m at a point where all of the app works and does what I want using the following products:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;EC2&lt;/li&gt;
&lt;li&gt;ElastiCache&lt;/li&gt;
&lt;li&gt;Route53&lt;/li&gt;
&lt;li&gt;CodeCommit&lt;/li&gt;
&lt;li&gt;SES&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and am already re-thinking some things that could change or that could be re-done. One thing I&amp;rsquo;ve been toying around with was using the Lambda service to get rid of servers but since the app is already written in Go I would need to re-write everything. I&amp;rsquo;d also like to wrap in the CodeDeploy and CodePipeline services since I only have a single app and would be able to stay on the free tier.&lt;/p&gt;

&lt;h3 id=&#34;so-when-aws-goes-under:028da4a231a5317af863941c3c91d223&#34;&gt;So when AWS goes under&lt;/h3&gt;

&lt;p&gt;One of the biggest risks with this is that even though I haven&amp;rsquo;t put any AWS specific API calls or anything into my code, if AWS was going to close at the end of the month I would have some problems. The biggest of course would be replicating the services on my own hardware or VMs. It&amp;rsquo;s easy enough to just setup a basic SMTP box or HAProxy load balancer, the complications come into play when you start talking about multiple availability zones and regions. Keeping things simple while allowing yourself to scale with a push of a button is something that AWS does a really good job of and that is not easy to replicate. My personal belief is to develop your application(s) to use non-vendor specific APIs or whatever so that you can easily move it if needed, but until that point comes leverage the offerings your vendor has and spend the time you&amp;rsquo;re saving to improve your application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Elasticsearch in Production, at Scale: One Man&#39;s Story</title>
      <link>http://esheavyindustries.com/2015/03/elasticsearch-in-production-at-scale-one-mans-story/</link>
      <pubDate>Tue, 24 Mar 2015 18:43:05 -0600</pubDate>
      
      <guid>http://esheavyindustries.com/2015/03/elasticsearch-in-production-at-scale-one-mans-story/</guid>
      <description>

&lt;h4 id=&#34;in-the-beginning:f90d37087c90d70a24c8fee54358486d&#34;&gt;In the Beginning&amp;hellip;&lt;/h4&gt;

&lt;p&gt;A few years ago I was part of a team that had a logging problem. We were part of an older style architecture and were charged with supporting all of the &amp;ldquo;middleware&amp;rdquo; which in this case, was pretty much everything except the databases or the physical servers.
This company was a fairly large company, one of the leaders in it&amp;rsquo;s industry so we dealt with a wide range of &amp;ldquo;middleware&amp;rdquo; services which each had different levels of criticality and volume. Everything from looking up a customer&amp;rsquo;s account details to submitting a request to have a return shipping box sent to the customer was in our world. Each one of these calls were logged in some form or fashion.
We actually had quite verbose logging as a lot of this information was gathered on a daily basis by our development teams who would then run a bunch of manual scripts and applications against the data. For that reason it was not unusual for a log message to be 2500kb or larger in some cases. Of course we also had the usual 100kb logs but I&amp;rsquo;d say our typical range was from 500-1500kb.&lt;/p&gt;

&lt;p&gt;Oh yeah, we had a ton of these coming through (as we&amp;rsquo;d find out later). In total we later came to find out we were doing about 1.5-2 terabytes of logging a day in production. All of this is to set the stage for the rest of this entry. I want you to understand the domain we were in as at the time, it was hard for us to find anyone doing near the amount of data we were with Elasticsearch. That&amp;rsquo;s not to say it wasn&amp;rsquo;t being done, the info just wasn&amp;rsquo;t being made public.&lt;br /&gt;
Sadly due to contractual obligations I am only now able to share how we solved our logging problem. I realize it has been a few years and things have changed but I think the core ideas and design behind what we achieved would still apply today. Plus I enjoy reading posts/stories from people that provide detail on how they solved a certain problem. It doesn&amp;rsquo;t always have to be the right one, but just how they did it.&lt;/p&gt;

&lt;h4 id=&#34;first-attempt:f90d37087c90d70a24c8fee54358486d&#34;&gt;First Attempt&lt;/h4&gt;

&lt;p&gt;So we had a logging problem, how were we going to solve it? Whatever we came up with had to be searchable, had to be near real-time and it had to scale.
We already had a product from a third party that we had paid a ton of money for that didn&amp;rsquo;t even work. The first time we tried it on just one of our servers it choked and gave up within 30 minutes so we knew that was not an option. Most people know about Splunk so we gave them a call and quickly realized that in order to support what we wanted we would end up backing an armored bank trunk up to their office every month. Needless to say they were not an option.&lt;/p&gt;

&lt;p&gt;I had some toy code from a previous project that had a concept of log parser -&amp;gt; queue -&amp;gt; log consumer -&amp;gt; database. This had never really gone anywhere but I felt that the basic design was fairly sound so I pitched it and was given a few weeks to pull something together to determine if this was possible.
One thing that was working in our favor is that we had just recently built a shiny new Apache Kafka cluster. This was for another project but was built to support IT as a whole so it was pretty beefy and just based on the nature of Kafka, scalable and redundant. Yes, we are dealing with &amp;ldquo;just logs&amp;rdquo; but these are production logs and we really don&amp;rsquo;t want to lose them.&lt;/p&gt;

&lt;p&gt;Ok cool, we know what the queuing system is going to be, what&amp;rsquo;s next? I honestly can&amp;rsquo;t remember why but for some reason the first decision was to use MongoDB + Elasticsearch, coupled together with a MongoDB river. Our logs would be inserted into MongoDB but we would use Elasticsearch for the indexing/searching functionality. This is where the story flow jumps around a bit because we did end up building out a POC with this back-end setup only to ditch it later.&lt;/p&gt;

&lt;p&gt;The main reason we ditched MongoDB + Elasticsearch was really just the un-necessary complexity that MongoDB added. We realized that it made sense to just treat Elasticsearch as our database and store everything directly in there, so that&amp;rsquo;s what we did.
Now we have a queuing application (Kafka) and a &amp;ldquo;database&amp;rdquo; (Elasticsearch), the next challenge was getting the log data into Elasticsearch. Our end goal was to have our applications write directly to Kafka and we even hacked up a simple Kafka log4j appender, but the powers that be were not comfortable with making such a large jump. It was decided that for our first trial phase we would keep writing logs to log files and then parsing those log files and sending the data to Elasticsearch.
This was before logstash became part of the Elasticsearch umbrella but it was still being described as a great pairing. The ELK stack as it came to be known was out there, it just came from three different people. We took a look at logstash but immediately hit a roadblock: it had no support for Kafka (at that time, I believe it does now). Logstash is written in ruby/jruby/whatever and for no real reason I had never really had an interest at writing ruby code. I can&amp;rsquo;t say anything bad about it because I honestly have never written one line of code in it, but at that moment I didn&amp;rsquo;t feel that learning ruby to write an add-on for something that might not even work for us was the best use of time, so we dropped logstash as an option.&lt;/p&gt;

&lt;p&gt;What did we end up going with? Our own in-house java applications that I really wish could have been open sourced but again, the people writing the checks were not really open to that idea. We ended up writing what we called our &amp;ldquo;log parser&amp;rdquo; that lived on the servers and sent the data from the log files to Kafka and then a &amp;ldquo;log consumer&amp;rdquo; that took the data off of Kafka and inserted it into Elasticsearch. Sounds simple enough right? Not so much, or at least not for us.&lt;/p&gt;

&lt;h4 id=&#34;second-attempt:f90d37087c90d70a24c8fee54358486d&#34;&gt;Second Attempt&lt;/h4&gt;

&lt;p&gt;We now have our technology stack at least picked out so it was time to start putting it together and see what we could do. There were a few design considerations that went into place around the parts we were creating. The main purpose of our log parser was to get data from the servers into Kafka as quickly as possible. We did some very loose formatting and clean-up of message but that was it. The idea was to spend as little time putting data into Kafka because that process was something we couldn&amp;rsquo;t really scale. We could always (at least that&amp;rsquo;s how we would design it) add more log consumers to keep up with incoming traffic but we couldn&amp;rsquo;t really add more log parsers to speed up the parsing -&amp;gt; Kafka stuff.&lt;/p&gt;

&lt;p&gt;The log parser itself was fairly basic, nothing super crazy outside of some simple regular expressions and threading. We ran multiple applications per physical server so we wrote it in a way that a single log parser could handle all logs on that server. This was setup via a config file which controlled what logs were monitored. From that point on it was basically just doing a &amp;ldquo;tail -F&amp;rdquo; on the logs and each time something was written it massaged it a bit and sent it on to Kafka.&lt;/p&gt;

&lt;p&gt;If the log parser was fairly simple the log consumer was ummm, a bit less simple. Since we had gone for speed on the log parser most of the heavy work was left to the log consumer. The good thing about our design was that we could horizontally scale this part in order to handle increased volume or just plain inefficiencies. For this round we knew there were a few things we wanted to break out of the message into their own Elasticsearch fields/terms. Things like the date, log level, class name, etc. In addition to that we had the log message itself which usually contained a bunch of other fields which bit us later on (more on that later). Being a big company with various development teams who all used their own logging formats we ended up with a mess of regex strings that we tested messages against to figure out where the fields we cared about were. On the surface this sounds really heavy and not very manageable, and at first it was. What we ended up building though was kind of a regex engine for only things we cared about that we could configure with a config file and that kept all of the compiled regular expressions in memory. It was still less than ideal but it got us what we needed and did it pretty quickly. Oddly enough I found out later that when they tried to replace the log parser with a version of logstash that supported Kafka, logstash was actually quite a bit slower. I don&amp;rsquo;t think this is a knock against logstash, it&amp;rsquo;s just that we had created essentially a customized, stripped down version of logstash built for only our logging environment.&lt;/p&gt;

&lt;p&gt;At this point we were in cruise control. All of the hard work had been done, we were able to pull messages off of Kafka as quickly as we could put them on and we had already started planning the celebration party. We just had to make the final hook into Elasticsearch and we&amp;rsquo;d be done. No problem, just toss together an Elasticsearch cluster, point the log consumers at it and sit back and watch. Wrong. Wrong.&lt;/p&gt;

&lt;p&gt;Elasticsearch is an awesome product, honestly. It&amp;rsquo;s one of the few apps I have used that takes a complex task and makes it super simple &lt;strong&gt;AND&lt;/strong&gt; it typically just works right out of the box. I was lucky enough to attend one of their first training classes and it&amp;rsquo;s obvious those guys/girls know their shit. Unfortunately for us we did not have any real world examples to study so when we built our Elasticsearch cluster from a random SWAG it totally missed the mark. Our consumption rates dropped a ton as soon as we pointed the log consumers at Elasticsearch and we were lucky to get a steady 1000 messages a second inserted. At first we thought it was the log consumers but as we researched it more we found the bottleneck was Elasticsearch. If I remember right I think we had four beefy indexing nodes (I want to say 120+gb of memory, a billion cores, etc) and then two &amp;ldquo;search&amp;rdquo; nodes that were a bit lighter.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s probably as good a time as ever to point out that a lot of the knowledge that Elasticsearch has provided users didn&amp;rsquo;t exist back then. That&amp;rsquo;s not anyone&amp;rsquo;s fault or a mark against Elasticsearch, it&amp;rsquo;s just what it is when it comes to open source products. Usually the documents are the last thing to get updated and given that Elasticsearch was still kind of flying low at this point, I don&amp;rsquo;t think a lot of people were pushing production traffic through it like we were so nobody was talking about the issues we were running into.&lt;/p&gt;

&lt;p&gt;Celebration party is cancelled, everyone is predicting failure and management is going on again about why we shouldn&amp;rsquo;t use open source products. In an attempt to right the ship we did exactly what you shouldn&amp;rsquo;t do with Elasticsearch: start turning random knobs and dials. We read through the documentation looking for any setting that sounded like it could potentially help and &amp;ldquo;tuned&amp;rdquo; it to some random number we pulled out of our ass. Sometimes it would improve things, but not drastically and usually it just made things worse. Another cool thing about Elasticsearch (we found out later) is that it has a lot of internal monitors to adjust/set things as it needs to based on the environment it&amp;rsquo;s currently in. We learned later that hardly ever do you need to change config settings outside of the basic stuff (networking, cluster name, etc). This is something that is now all over their site :)&lt;/p&gt;

&lt;p&gt;What do IT people do when they&amp;rsquo;ve run out of options and have no idea what they are going? Head to google.com! We spent a day or two searching for everything we could without any luck and eventually posted up a plea for help on the Elasticsearch mailing list. We got a few tips but nothing that we had already tried and again, the biggest issue was that nobody that was willing to respond had ever dealt with the volume we were doing. Luckily after a few days one of the nice Elasticsearch people reached out to me via E-mail to see how our testing was going. I think I had given my e-mail address on their website somewhere so it was pure luck that they reached out to me in the middle of our crisis. I explained the situation, provided the link to our mailing list posting and asked if they had any ideas. They actually got back to us and were willing to do a brief call and go over the issues we were seeing. Now I realize they are probably not going to do this for everyone who downloads their product, but at the time it was a glimmer of hope and we took full advantage of it. The engineer on the call gave us some great tips, all of which are on their site now, that did help out but it still didn&amp;rsquo;t get us where we needed to be. The bottom line was that we had a scaling problem. We simply did not have enough Elasticsearch resources to handle the data we were sending in so we needed to scale.&lt;/p&gt;

&lt;h4 id=&#34;third-attempt:f90d37087c90d70a24c8fee54358486d&#34;&gt;Third attempt&lt;/h4&gt;

&lt;p&gt;We&amp;rsquo;re all feeling better now that we&amp;rsquo;re not total failures and that we have a solvable solution. The question was how much was it going to take to solve it. One of the tips that came from our call with Elasticsearch was to shrink our heap down a bit and avoid using the G1 collector. They also mentioned that you typically want to dedicate half of your memory to file system cache (again, this is all on their site now). We ended up setting our heap at 30gb which in turn allowed us to run two Elasticsearch nodes on one box while still meeting their suggestions. In one config change we had just doubled our cluster size.
The next change we made was moving to local disks instead of using our SAN. I can&amp;rsquo;t tell you what SAN we were using or what the infrastructure looked like between the servers and the SAN, but that single change gave us the most improvement overall. Before we went to local disks we tried all types of kernel and scheduler tuning and just couldn&amp;rsquo;t get the numbers we needed. We ended up just putting in a shit ton of disks and partitioned them up into two groups, one group for each Elasticsearch node running on the server. No special RAID or anything, just a bunch of plain, formatted mounted disks and we let Elasticsearch do the load balancing if you will across them.&lt;/p&gt;

&lt;p&gt;Now we were getting somewhere! We were able to sustain 20k messages a second and handled spikes up to around 40k a second. We were basically consuming log messages near real-time (15ish second delay usually) and things seemed to be humming along. The celebration party was re-scheduled and everyone started giving high fives.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Time in GC is going up really fast&amp;rdquo; is something you don&amp;rsquo;t like to hear when supporting Java apps but that&amp;rsquo;s what I had just heard my co-worker tell me. He was talking about one of our Elasticsearch nodes and my heart sank. It turns out that while we had solved the indexing/input problem, we had never up to this point really used the search portion much. Previously the app was in such flux that most of the team had not been using it but now that it was stable, everyone was using it. Even people from other teams. Teams who wanted business data for an entire month based on a single term that tended to be in those 2000+kb log messages. Those familiar with Elasticsearch can probably already tell where this is going but basically we did not have enough resources in our cluster to support intense queries like this. The behind the scenes stuff that happens with Elasticsearch/Lucene based on these huge wildcard type searches were choking the cluster. We named this style of query &amp;ldquo;the query of death&amp;rdquo; because it always brought something down. You have to remember that we are talking over a terabyte of data per day x ~30 = lots of shit that needs to be put into memory.&lt;/p&gt;

&lt;p&gt;By this point our &amp;ldquo;couple of weeks&amp;rdquo; project had been going on for 3+ months and although the initial hardware we were given was free in the sense that it came from a retired project, we were a bit hesitant to go back to the well asking for more machines. The short-term solution was two pronged. First we had to educate our users on how to more efficiently use Elasticsearch + Kibana. For the IT side this wasn&amp;rsquo;t too big of a challenge but on the business side (how did they even end up in this discussion? These are system logs after all) it was a little bit more difficult.
They wanted a Google like search that just worked and with our current setup that wasn&amp;rsquo;t possible yet.
The second part of our short-term solution was to take our storage from 30 days down to 7. We were keeping a daily index in Elasticsearch so we just adjusted our clean-up script to basically set a hard limit on how much data you could shove into a query. This seemed to keep everyone happy for a bit and got us some extra time to research a bit more on other possible solutions. The end goal had always been to have 30 days of data so 7 days was not a permanent solution.&lt;/p&gt;

&lt;p&gt;I think it was around this time that I stumbled across a video from the Loggly guys where they had done a talk about their infrastructure. After watching their video I felt a bit of relief. We were starting to wonder if what we were doing was even possible with our setup, only to see this video where a real company who makes real money selling their logging product had an infrastructure almost identical to ours. They of course have their own custom log parser and log consumer, but they were doing the whole Kafka and Elasticsearch dance, plus they were doing it on AWS. If they could do it on AWS we should surely be able to do it with physical hardware.&lt;/p&gt;

&lt;p&gt;Back to solving our search problem&amp;hellip; The first thing we tried to do was go after some of the more common terms that people were searching/aggregating on and pull those out into their own terms. This way Elasticsearch didn&amp;rsquo;t have to load up the entire message when people from the business were doing their monthly reports or creating their business metric dashboards. We got a list of the most common terms they were searching on and went to work on the log consumer adding in even more regex-fu to help build built out more fine-grained documents for Elasticsearch. This helped us out when it came to the queries we knew were coming our way, but we couldn&amp;rsquo;t account for every strange query that was going to be sent so the only real solution was to expand our cluster. Again.&lt;/p&gt;

&lt;h4 id=&#34;fourth-attempt:f90d37087c90d70a24c8fee54358486d&#34;&gt;Fourth attempt&lt;/h4&gt;

&lt;p&gt;Unfortunately it was at this point where my time with this company came to an end. It was a bittersweet moment as this project had been one of the most challenging but rewarding projects I had been apart of. I did keep in touch with a few of the people I worked with on the project and found out that they were able to essentially double the size of the cluster and go back to 30 days worth of storage. The query of death would still pop up but it was not a weekly event anymore and it was something that was easily handled.
I realize that is a fairly abrupt end to the story without a real success or failure indication but hopefully it will help someone out there who is researching building their own logging solution for terabytes of data a day.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unleash the Hounds, err, Bees?</title>
      <link>http://esheavyindustries.com/2015/03/unleash-the-hounds-err-bees/</link>
      <pubDate>Wed, 18 Mar 2015 22:10:39 -0600</pubDate>
      
      <guid>http://esheavyindustries.com/2015/03/unleash-the-hounds-err-bees/</guid>
      <description>

&lt;h3 id=&#34;killabees-on-the-swarm-literally:3c794f4bfe616a5304fbe2573b9e44c8&#34;&gt;Killabees on the swarm, literally&lt;/h3&gt;

&lt;p&gt;At work we are running most of our applications in Docker Swarm. So far it has been mostly good and has given us enough confidence in it to continue moving forward. If you are familiar with Docker you know that you basically can use &amp;ldquo;pure&amp;rdquo; Docker and manage it via it&amp;rsquo;s API or command line tools OR you can use one of the many products that have sprung up around Docker that run on top of it offering a whole host of services. Examples of these would be &lt;a href=&#34;http://mesosphere.com/&#34;&gt;Mesosphere&lt;/a&gt;, &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;, the whole &lt;a href=&#34;https://coreos.com/&#34;&gt;CoreOS&lt;/a&gt; stack, etc.
These are all great products and at one point we were actually running a small test cluster on Mesosphere, but for us and what we wanted, they were just a bit too much. I personally am a bit scared by all of the &amp;ldquo;magic&amp;rdquo; that goes on with most of these products and the complexity that comes with them. On the surface it doesn&amp;rsquo;t seem like much and when things work you forget about it, but when things start heading south and you start having issues it can become a nightmare to track down where the issue is coming from and what is causing it. Assuming you do find the issue, is it something that will have a cascading impact if you try to fix it (fixing A breaks B)?&lt;/p&gt;

&lt;p&gt;Everyone wants reliability but our particular suite of products really do need to be reliable (five 9&amp;rsquo;s is our typical SLA). For us, having a lot of complexity on top of a fairly complex product already (Docker) wasn&amp;rsquo;t really making us feel warm and fuzzy so we set out to create our own. The idea was not to go up against something like Mesosphere or Kubernetes, but instead create some tools that are lightweight, do just what we need and are for the most part, standalone. What we came up with is a group of tools that I&amp;rsquo;m calling the killabees (swarm, bees, get it?)&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;figure&gt;
  &lt;img src=&#34;http://esheavyindustries.com/pics/killabees.png&#34; alt=&#34;bees!&#34;&gt;
  &lt;figcaption&gt;&lt;b&gt;comin atch&amp;rsquo;ya!&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;Currently the killabees consists of three tools:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/radiantiq/deploy-o-matic&#34;&gt;Deploy-o-matic&lt;/a&gt;: used for deploying containers and populating VIPs&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/radiantiq/flipper&#34;&gt;Flipper&lt;/a&gt;: used to flip containers from the &amp;ldquo;cold&amp;rdquo; VIP to the &amp;ldquo;hot&amp;rdquo; VIP&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/radiantiq/conf-builder&#34;&gt;Conf-builder&lt;/a&gt;: used to update HAProxy when VIP members change&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The only requirement for this to work outside of Docker? &lt;a href=&#34;https://consul.io/&#34;&gt;Consul&lt;/a&gt; and &lt;a href=&#34;http://www.haproxy.org/&#34;&gt;HAProxy&lt;/a&gt; which I&amp;rsquo;m willing to bet most of you have running already.
We use Consul to track what containers belong to what VIP and then we use HAProxy to act as the front for those VIPs. To explain how it actually works it&amp;rsquo;s probably easier to just pull a blurb from the &lt;a href=&#34;http://radiantiq.github.io/deploy-o-matic/&#34;&gt;deploy-o-matic page&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;We use a deployment processes based around the Netflix red/black design. In this case, red is our customer facing containers (live, production traffic) and black is our standby set of containers but they are still in production. Basically red = hot, black = cold.&lt;/p&gt;

&lt;p&gt;Deploy-o-matic by default will deploy all new containers to the black VIP. This will allow you to do any testing or final shakedowns that you want. Once you are comfortable that everything looks good, you can use another tool from our stack called &amp;ldquo;flipper&amp;rdquo; to copy the containers in the black VIP into the red VIP.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;We wanted to automate as much of the heavy lifting as possible but still be able to look in on the process as it moved along. Since each one of these steps is actually done with a different tool, it&amp;rsquo;s fairly painless to chain them together in a fully automated fashion if that&amp;rsquo;s what you&amp;rsquo;re after.&lt;br /&gt;
The best part (in our minds) is that it&amp;rsquo;s dead easy to troubleshoot. We are only using the Docker API and the Consul API with some logic wrapped around it. If something breaks it&amp;rsquo;s either our code or Docker/Consul and the error message is likely to indicate which one it is.&lt;/p&gt;

&lt;p&gt;As noted in most of the github repos for these projects, they are in a very early phase so the code is pretty crazy and there are not as many safety checks as there should be. With that being said though we have successfully deployed to our production clusters many times using the same code that is in github. Things are not where we want them to be yet but the code does run and do what it&amp;rsquo;s supposed to do.&lt;br /&gt;
The one thing missing for us that is on our list is an auto-scaling type daemon that tracks what containers are running and deploys more as needed based on set thresholds. This will again be it&amp;rsquo;s own separate tool using only the Docker API and Consul. Once we have something that is working we&amp;rsquo;ll probably be throwing it up on github as well so keep an eye out if you&amp;rsquo;re interested.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amazing Chaos aka Parenthood</title>
      <link>http://esheavyindustries.com/2015/02/amazing-chaos-aka-parenthood/</link>
      <pubDate>Wed, 25 Feb 2015 20:34:11 -0700</pubDate>
      
      <guid>http://esheavyindustries.com/2015/02/amazing-chaos-aka-parenthood/</guid>
      <description>&lt;p&gt;&lt;em&gt;this post stems from reading horrible news stories, multiple days in a row, about young children dieing because of parental neglect and/or abuse and wondering how someone could do something so awful&amp;hellip;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;ldquo;The hardest job you&amp;rsquo;ll ever love&amp;rdquo; is a perfect phrase describing what parenting is to me. I have dealt with quite a wide range of things in my life and none of them even come close to the difficulty of the first 3 months with my son. Sure, everyone knows about the lack of sleep but what about the stress? You are now responsible for a living, breathing person that is 100% dependent on you for survival.
Oh yeah, this new person also has no real way of communicating with you what it is they actually need. If you are a &amp;ldquo;type a&amp;rdquo; person then you are in for even more of a ride.
Forget having any control or structure over anything. You can try, and you might get a few wins here-and-there, but overall you are in a world of constant change where things change by the hour so buckle up!
Did I mention there are no owners manuals either? Something as simple as how to give this new person you have a bath becomes rocket science that only adds to that stress you have building in you.
Now back to that sleep thing (ha!), all of this is happening while you are literally running on empty. It&amp;rsquo;s not only the fact that you are only sleeping a couple of hours a night but also that those hours are typically not in consecutive order.&lt;/p&gt;

&lt;p&gt;You know how I currently spend my weekends? Sitting around with my wife watching our child learn, play, laugh and cry. That&amp;rsquo;s it. I literally will spend most weekends just sitting there on the floor watching him as he does what small children do (which is everything!) and I don&amp;rsquo;t regret it one bit, in fact there have been multiple times where I thought to myself &amp;ldquo;this is awesome&amp;rdquo;.
Sure there are times when I think it would be nice to be able to just go do something on a whim without any planning but those occasions are rare and quickly brushed aside when I see him and am filled with an overwhelming feeling of love.
The love that I have for him is something that cannot be described and can only be felt. It is something that I have never felt in all of my life.
&lt;br&gt;It doesn&amp;rsquo;t end there though. &lt;br&gt;
The feeling of pride that comes over you when your child does something for the first time on their own is just incredible. If you could capture that feeling somehow and sell it you would be a billionaire. You are watching this child learn EVERYTHING right before your eyes.
This is not someone learning a new hobby or something, they are literally learning how to survive and become their own person.
Yes, it&amp;rsquo;s a great feeling when you get a raise at work or when your team wins but it doesn&amp;rsquo;t even come close to seeing your child walk for the first time or the first time they call you &amp;ldquo;dada&amp;rdquo; or &amp;ldquo;mama&amp;rdquo;.
It&amp;rsquo;s like the biggest team accomplishment ever as all of you have been putting in work to help reach this goal. Your child of course has been doing the majority of this work, surely falling and stumbling along the way, but has managed to overcome all of the obstacles and has now taken another step towards independence. The best part is that these moments are going to just keep coming as your child grows and gets older.&lt;/p&gt;

&lt;p&gt;You are committed 100%, total dedication to this new human. Sure, there are &amp;ldquo;selfish&amp;rdquo; breaks where you step outside those lines for a bit. A night out with friends here, a large non-baby item purchase there, but that doesn&amp;rsquo;t change your focus or your goals for your child. From here on out everything you do is for them.
&lt;br&gt;And I don&amp;rsquo;t mind it at all. &lt;br&gt;
For me being able to hopefully provide for my son whatever he needs or wants (within reason) is a great motivator and something I feel that I owe him. He has had such a drastic impact on my life that all I want to do is make sure he is happy. My hope is that his mother and I can raise him in such a fashion that if he ever has a child of his own, the foundation we established for him will allow him to raise a child in the same manner.
It has been quite the shift in mindset as now I am always thinking about what I will leave behind instead of what is going on right now. A few years ago I couldn&amp;rsquo;t tell you what was happening next week but now I have plans for 30 years from now and it&amp;rsquo;s all because of one tiny (currently) person.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wrangling Zookeeper</title>
      <link>http://esheavyindustries.com/2015/01/wrangling-zookeeper/</link>
      <pubDate>Sat, 31 Jan 2015 22:02:17 -0700</pubDate>
      
      <guid>http://esheavyindustries.com/2015/01/wrangling-zookeeper/</guid>
      <description>

&lt;h5 id=&#34;spof:12b272660e693d03fef595b262170b60&#34;&gt;SPOF&lt;/h5&gt;

&lt;p&gt;SPOF = Single Point of Failure and we had one. It wasn&amp;rsquo;t a traditional failure point like having a critical service running on a single server or something, it was the fact that we had no way to auto-scale, or at least automate spinning up, one of our key components: &lt;a href=&#34;http://zookeeper.apache.org/&#34;&gt;Apache Zookeeper&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;figure&gt;
  &lt;img src=&#34;http://esheavyindustries.com/pics/zookeeper_small.gif&#34; alt=&#34;zookeeper&#34;&gt;
  &lt;figcaption&gt;&lt;b&gt;smug bastard&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;As we had been drawing up the architecture for our new environment one of the requirements is everything must auto-scale or be push button. Since we will be working in AWS, the architecture has been designed to handle failures across the board. Having something that can&amp;rsquo;t self heal or be re-provisioned without human intervention is dumb and would break our entire goal. There have been too many times where I&amp;rsquo;ve heard &amp;ldquo;So everything is automated&amp;hellip; except this one part that fails all the time and we have to pay a team to monitor 24 hours a day. Oh and by the way that part that fails all the time is key to everything else working&amp;rdquo;. Unfortunately Zookeeper is a bit old in it&amp;rsquo;s ways and still requires a few areas of manual configuration and taking it out of our stack was not an option so what options did we have?&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Accept that we couldn&amp;rsquo;t automate this part and that despite our grand idea of what could be we would just build the same hodgepodge of shit everyone else had.&lt;/li&gt;
&lt;li&gt;Have a go with &lt;a href=&#34;https://github.com/Netflix/exhibitor&#34;&gt;Exhibitor&lt;/a&gt;. Netflix built it so it&amp;rsquo;s gotta be good right?&lt;/li&gt;
&lt;li&gt;Build our own solution&lt;/li&gt;
&lt;li&gt;Watch cat videos on youtube&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A combination of options 3 and 4 is what we ended going with (actually option 4, option 4 some more and then option 3. That led to some more of option 4). Why not option 2? While I&amp;rsquo;m sure Exhibitor is a fantastic app, it just didn&amp;rsquo;t feel right to us. The ideal way to use Exhibitor is with S3 which means we are relying on an AWS service. Our goal is to handle failures across the board so if we have a Zookeeper EC2 instance fail and a regional S3 failure we&amp;rsquo;re fucked. We could probably build around that but it just adds another stack of shit to keep track of. Additionally with Exhibitor all of the Zookeeper management is now done through Exhibitor. This could be a good thing I guess but we didn&amp;rsquo;t want yet another management tool being thrown into the mix.&lt;/p&gt;

&lt;p&gt;Ok, enough gum flapping, here is how we solved the question of the century: how do you automate scaling Zookeeper?&lt;/p&gt;

&lt;p&gt;Enter &lt;a href=&#34;https://github.com/coreos/etcd&#34;&gt;etcd&lt;/a&gt; and &lt;a href=&#34;https://github.com/kelseyhightower/confd&#34;&gt;confd&lt;/a&gt; (plus some custom &lt;a href=&#34;http://golang.org&#34;&gt;Go&lt;/a&gt;)!&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;figure&gt;
  &lt;img src=&#34;http://esheavyindustries.com/pics/jackpot.jpg&#34; alt=&#34;jackpot&#34;&gt;
  &lt;figcaption&gt;&lt;b&gt;blammo!&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;We had already been using etcd for service discovery and minor random things we probably shouldn&amp;rsquo;t be using it for so it was already part of our stack. Additionally our etcd groupings had already been built so they fit our requirement of being able to handle catastrophic failure. Confd had been on the radar for another portion of our architecture so we were familiar with it and thought we&amp;rsquo;d toss it in the mix and see what happens. What happened was pure magic! The ability to add and remove Zookeeper instances without any human configuration and no additional AWS services to lean on. I guess it wasn&amp;rsquo;t pure magic but it was pretty cool.&lt;/p&gt;

&lt;p&gt;So how does this whole thing work? I&amp;rsquo;m glad you asked:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First we wrote a startup wrapper in Go for Zookeeper. At a high level what this wrapper does is control the &amp;ldquo;myid&amp;rdquo; file for the Zookeeper instance. There are various checks in there to make sure that one instance only gets one, previously unused ID. This information is published to etcd. On first start there is no cluster config so this new instance has an id but isn&amp;rsquo;t actually joined to anything.&lt;/li&gt;
&lt;li&gt;After we had figured out how to publish the information for everyone, we wrote up a quick confd template that builds out the &amp;ldquo;zoo.cfg&amp;rdquo; cluster details. This is the lame part because the number associated with the entry must match the &amp;ldquo;myid&amp;rdquo; of that host. So when you see &amp;ldquo;server.2=zoo2:2888:3888&amp;rdquo; in the config file, zoo2 better have an id of 2 or else shit might go down (literally). Once the config has been populated confd restarts Zookeeper. Confd is always checking in with etcd and updating the &amp;ldquo;zoo.cfg&amp;rdquo; file as needed.&lt;/li&gt;
&lt;li&gt;We have another tiny Go app that when it has free time, checks the entries in etcd to make sure they are accurate and removes any dead hosts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So yeah, that&amp;rsquo;s about it, nothing earth shattering by any means but a possible solution for some of you who are looking to do the same thing and for whatever reason are not interested in Exhibitor.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>So long and thanks for all the shoes...</title>
      <link>http://esheavyindustries.com/2015/01/so-long-and-thanks-for-all-the-shoes.../</link>
      <pubDate>Sun, 11 Jan 2015 21:15:54 -0700</pubDate>
      
      <guid>http://esheavyindustries.com/2015/01/so-long-and-thanks-for-all-the-shoes.../</guid>
      <description>&lt;p&gt;It&amp;rsquo;s kind of strange to start off a new blog with a goodbye post&amp;hellip;&lt;/p&gt;

&lt;p&gt;After many years of living in the world of large corporations and the herd mindset I am going back to doing what I love: &lt;strong&gt;building shit&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;No more operating in a culture where risk is looked down upon and the people who take those risks (or just suggest taking them) are typically not welcomed or are ignored. No more working in an environment where mediocrity is rewarded and innovation is stifled.&lt;/p&gt;

&lt;p&gt;Instead I&amp;rsquo;ll be moving to an environment where innovation is a first class citizen and is one of the keys to success. An environment where the majority of your day is not spent in meetings talking about building things but instead is actually &lt;em&gt;&lt;strong&gt;spent building those things&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll be going back to early in my career and joining, what I believe, is an awesome stealth mode start-up where I will be working with a small team as we build everything from the ground up. Not only do I get to work with really smart people who care about what they are building, but I get to help develop the right technology for the job instead of getting someone else&amp;rsquo;s old crap that was selected because the vendor took them out for drinks or something.&lt;br /&gt;
So hello &lt;a href=&#34;http://zachholman.com/talk/move-fast-break-nothing/&#34;&gt;move fast and try not to break things&lt;/a&gt; and goodbye &lt;a href=&#34;http://en.wikipedia.org/wiki/Groundhog_Day_%28film%29&#34;&gt;ground hog day&lt;/a&gt;!&lt;br /&gt;
&amp;ldquo;We don&amp;rsquo;t have a lot of time on this earth! We weren&amp;rsquo;t meant to spend it this way! Human beings were not meant to sit in little cubicles staring at computer screens all day, filling out useless forms and listening to eight different bosses drone on about mission statements!&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>